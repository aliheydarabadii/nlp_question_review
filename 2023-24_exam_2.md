# 2023-24: Natural Language Processing -- Exam 2 Analysis

This document provides a detailed question-by-question analysis of the exam, explaining the correct answer for each and the reasoning behind why other options are incorrect.

---

### **Question 1**
**Question:** Which of the following statements about the most frequently occurring words in a corpus is true?
**Correct Answer:** NONE of the other answers are correct

*   **Analysis:** The most frequently occurring words in a corpus (e.g., "the", "a", "is", "in") are known as **stop words**.
    *   `they are the most important words in the query`: **Incorrect.** Stop words carry very little semantic meaning and are usually the *least* important words for determining the topic of a query or document.
    *   `they result in short posting lists`: **Incorrect.** A posting list for a term is the list of all documents that contain it. Since stop words appear in almost every document, they have the *longest* posting lists.
    *   `they should never be removed during preprocessing`: **Incorrect.** They are very frequently removed during preprocessing (a step called stop word removal) precisely because they are not informative and can add noise to many NLP tasks.
    *   `they are the most discriminative words in any document`: **Incorrect.** They are the *least* discriminative. A discriminative word helps distinguish one document from others; stop words do the opposite because they are common to all.
    *   Since all other statements are false, this is the correct choice.

---

### **Question 2**
**Question:** Low Rank Adaptation (LoRA) is a technique in which we:
**Correct Answer:** introduce a small set of new parameters in order to fine-tune an LLM.

*   **Analysis:** LoRA is a Parameter-Efficient Fine-Tuning (PEFT) method.
    *   **Correct:** LoRA's core idea is to freeze the massive number of pre-trained weights in an LLM and inject smaller, trainable "low-rank" matrices into the model's layers. This allows the model to be adapted to a new task by only training a tiny fraction of the total parameters, making fine-tuning much more efficient.
    *   `estimate word embedding vectors using low rank matrix factorisation`: **Incorrect.** This describes techniques for creating word embeddings from scratch, like Latent Semantic Analysis (LSA) or GloVe, not for fine-tuning an existing LLM.
    *   `adapt the loss function during training by reducing its rank`: **Incorrect.** LoRA modifies the model's weight matrices, not the loss function.
    *   `train a rank-learning algorithm to rerank retrieved documents in a RAG model`: **Incorrect.** This describes a "Learning to Rank" task, which is a component of a search system, not the LoRA technique itself.
    *   `approximate a document-term matrix with a low-rank factorisation...`: **Incorrect.** This describes topic modeling techniques like LSA.

---

### **Question 3**
**Question:** Which of the following statements about sub-word embeddings is NOT usually true?
**Correct Answer:** sub-word embeddings are sub-optimal with respect to full-word embeddings because they lose too much information.

*   **Analysis:** The question asks for the statement that is NOT true.
    *   **Correct (as the false statement):** This is not usually true. In fact, sub-word embeddings (like those used by BPE, WordPiece, or FastText) are often considered *superior* to full-word embeddings because they can handle words not seen during training (Out-of-Vocabulary or OOV words) and can better represent morphological variations (e.g., "run", "running", "ran"). The benefit of handling OOV words typically outweighs any minor loss of information from splitting words.
    *   `Fasttext is an example of subword embedding`: **True.** FastText represents words as a bag of their character n-grams, which is a form of sub-word information.
    *   `sub-word embeddings solve the problem of not being able to represent unseen (out-of-vocabulary) words...`: **True.** This is one of their primary advantages.
    *   `Tokenizers that employ byte-pair-encoding cause the resulting network to learn a sub-word embedding`: **True.** BPE is an algorithm to create a sub-word vocabulary, and the model then learns embeddings for these sub-word units.
    *   `sub-word embeddings split words up into smaller units and learn an embedding vector for each`: **True.** This is the fundamental concept.

---

### **Question 4**
**Question:** The size of the vocabulary grows roughly in proportion to the square root of the length of the document (/collection), is a statement of whose law?
**Correct Answer:** Heap's law

*   **Analysis:**
    *   **Correct:** This is the definition of **Heap's Law**. It models vocabulary size growth as a function of corpus size. The formula is `V = K * N^β`, where V is vocabulary size, N is corpus size, and β is a constant typically between 0.4 and 0.6 (close to the square root, i.e., 0.5).
    *   `Mandelbrot's law`: **Incorrect.** This is a generalization of Zipf's law, dealing with the frequency of words.
    *   `Murphy's law`: **Incorrect.** This is the adage "anything that can go wrong will go wrong."
    *   `Moore's law`: **Incorrect.** This relates to the growth of transistors in computer chips.
    *   `Zipf's law`: **Incorrect.** This describes the distribution of word frequencies (a few words are very common, many are rare), not the growth of the vocabulary.

---

### **Question 5**
**Question:** Which of the following Machine Learning models makes use of a bidirectional Transformer architecture to extract a feature representation of text?
**Correct Answer:** BERT

*   **Analysis:**
    *   **Correct:** BERT stands for **Bidirectional** Encoder Representations from Transformers. Its key innovation was using a Masked Language Model (MLM) pre-training objective, which allowed it to learn from both the left and right context of a word simultaneously.
    *   `GPT-2`: **Incorrect.** GPT models are unidirectional (or autoregressive), meaning they only process context from left-to-right to predict the next word.
    *   `Wav2Vec`: **Incorrect.** This is a model for speech processing, not text.
    *   `K-means`: **Incorrect.** This is an unsupervised clustering algorithm.
    *   The other options (`Naive Bayes`, `Word2Vec`, `SVM`, `XGBoost`) are other types of ML models, none of which are bidirectional Transformers.

---

### **Question 6**
**Question:** Given the piece of text "This exam is too much" and a trigram language model... What is the chance that the model produces the word "fun" as the next token if "top-k" sampling is used with k set to 5?
**Correct Answer:** 20%

*   **Analysis:**
    1.  **Identify the candidates:** Top-k sampling with k=5 considers the 5 most probable next words. From the list, these are: "work" (3%), "effort" (2.5%), "fun" (2%), "food" (1.5%), and "time" (1%).
    2.  **Sum their probabilities:** The total probability mass of this candidate set is `3 + 2.5 + 2 + 1.5 + 1 = 10%`.
    3.  **Renormalize:** In top-k sampling, the probabilities of the selected candidates are rescaled so they sum to 1. The new probability of picking "fun" is its original probability divided by the sum of the probabilities of all candidates.
    4.  **Calculate:** `P_new("fun") = P_original("fun") / P_sum = 2% / 10% = 0.20`.
    5.  This is equal to **20%**.

---

### **Question 7**
**Question:** Which statement about sequence-to-sequence models with attention is NOT true?
**Correct Answer:** they are chatbots that pay attention to the emotions of the user during the dialog.

*   **Analysis:** The question asks for the false statement.
    *   **Correct (as the false statement):** The "attention" mechanism in NLP is a mathematical technique that allows a model to weigh the importance of different parts of the input sequence when producing an output. It has nothing to do with the human concept of paying attention to emotions. This statement incorrectly anthropomorphizes the term.
    *   `they can be used to build machine translation systems`: **True.** This was the original and breakthrough application for these models.
    *   `they can be used to build automated question answering systems`: **True.** QA can be framed as a sequence-to-sequence task.
    *   `they allow information to flow (more) directly from input tokens...`: **True.** Attention creates direct "shortcuts" between the decoder and relevant encoder states, overcoming the bottleneck of a single fixed-size context vector in older RNN-based seq2seq models.
    *   `they allow for the reordering of corresponding tokens...`: **True.** The attention mechanism is very flexible and can easily learn to connect words that appear in different orders in the input and output, which is essential for translation between languages with different grammar.

---

### **Question 8**
**Question:** The picture provides a visualisation of which technique used with LLMs?
**Correct Answer:** Retrieval-augmented Generation (RAG)

*   **Analysis:** Without the image, we must infer its content from the correct answer. A standard diagram for RAG shows a process flow:
    1.  A user's query is received.
    2.  The query is used to `retrieve` relevant documents from a knowledge base (e.g., a vector database).
    3.  The retrieved documents are combined ( `augmented`) with the original query to form a new, more detailed prompt.
    4.  This augmented prompt is fed into an LLM to `generate` a final, context-aware answer.
    *   This workflow is unique to RAG and is a very common way to visualize it. The other options represent different concepts: `LoRA` is a fine-tuning method, `MoE` is a model architecture, `Quantisation` is a model compression technique, etc., all of which have different standard visualizations.

---

### **Question 9**
**Question:** The diagram shows an implementation of an attention mechanism. What names are usually given to A, B, and C?
**Correct Answer:** A: value, B: key, C: query

*   **Analysis:** The formula for scaled dot-product attention is `Attention(Q, K, V) = softmax( (Q * K^T) / sqrt(d_k) ) * V`.
    *   In a typical diagram, the **Query (Q)** and **Key (K)** vectors are used to compute a score for each input item. This score represents how much "attention" the current output should pay to that input item.
    *   These scores are then used as weights to compute a weighted sum of the **Value (V)** vectors.
    *   The mapping `A: value, B: key, C: query` fits this perfectly. The query (C) interacts with the keys (B) to produce weights, which are then applied to the values (A). The prompt's selection `A: value, B: key, C: query` seems to have a typo, but the intended logic maps to the standard Q, K, V roles. Based on the options, `A: value, B: key, C: query` is the correct association of the terms, even if the image's A, B, C placement might be ambiguous without seeing it.

---

### **Question 10**
**Question:** What is the difference between stemming and lemmatization?
**Correct Answer:** Stemming is a simple algorithm that applies rules to remove suffixes from word stems, while lemmatization is a more sophisticated technique that uses a dictionary and morphological analysis to extract the lemma.

*   **Analysis:**
    *   **Correct:** This is the standard, canonical definition.
        *   **Stemming:** A crude, fast, rule-based process that chops off endings. Example: `studies` -> `studi`. The result is not always a valid word.
        *   **Lemmatization:** A more complex, slower, knowledge-based process that considers the word's part of speech and uses a dictionary to find its root form (lemma). Example: `studies` -> `study`. The result is a valid word.
    *   `Lemmatization is a simple algorithm... while stemming is a more sophisticated...`: **Incorrect.** This reverses the definitions.
    *   `Stemming and lemmatization are exactly the same`: **Incorrect.** They are fundamentally different.
    *   `Stemming involves finding the STEM (Science Technology...) terms`: **Incorrect.** A humorous distractor playing on the acronym.
    *   `Stemming involves finding the stem word in the sentence...`: **Incorrect.** This misinterprets the linguistic term "stem" as a grammatical role like "subject".

---

### **Question 11**
**Question:** Latent Dirichlet Allocation (LDA) can be used to:
**Correct Answer:** discover topics discussed in a collection

*   **Analysis:**
    *   **Correct:** This is the primary purpose of LDA. It is an unsupervised generative model that assumes each document is a mixture of various topics, and each topic is a distribution of words. By analyzing a collection, it discovers these latent topics.
    *   `train a linear classifier...`: **Incorrect.** LDA is unsupervised; a linear classifier is a supervised model.
    *   `learn a Support Vector Machine`: **Incorrect.** An SVM is a specific type of supervised classifier.
    *   `learn word embeddings...`: **Incorrect.** While topics can be seen as a form of document/word representation, LDA is distinct from algorithms like Word2Vec or GloVe, which are designed to learn dense word vectors based on local context.
    *   `tokenise text within a document`: **Incorrect.** Tokenization is a preprocessing step that happens before applying a model like LDA.

---

### **Question 12**
**Question:** If the probability of the sequence "we really like NLP" was exactly 1/16, what would the perplexity of the sentence be? (Note: assume the sentence is tokenised at the word level.)
**Correct Answer:** 2

*   **Analysis:**
    1.  **Perplexity Formula:** Perplexity (PP) is `P(W)^(-1/N)`, where `P(W)` is the probability of the sequence `W`, and `N` is the number of tokens in the sequence.
    2.  **Identify N:** The sequence "we really like NLP" has `N = 4` tokens.
    3.  **Identify P(W):** The probability is given as `P(W) = 1/16`.
    4.  **Calculate:** `PP = (1/16)^(-1/4)`.
    5.  A negative exponent means taking the reciprocal: `PP = (16/1)^(1/4) = 16^(1/4)`.
    6.  The 4th root of 16 is 2 (since `2 * 2 * 2 * 2 = 16`).
    7.  Therefore, the perplexity is **2**.

---

### **Question 13**
**Question:** The fact that the expression "I made her duck" could mean "I caused her to lower her head to avoid being hit" or "I cooked the fowl that she had bought" is an example of the fact that:
**Correct Answer:** we often need to use surrounding context information to disambiguate the meaning of a sentence.

*   **Analysis:** This is a classic example of syntactic ambiguity.
    *   **Correct:** The sentence is ambiguous because "duck" can be a noun or a verb, and "made" can mean "caused to" or "cooked." To know the correct meaning, we need more context (e.g., "I made her duck when the ball flew past" vs. "I made her duck for dinner").
    *   `humans cannot communicate unambiguously...`: **Incorrect.** While ambiguity exists, context usually resolves it. This statement is too strong.
    *   `it is impossible to automatically processing...`: **Incorrect.** It's difficult, but modern NLP models are quite good at using context for disambiguation. It's not impossible.
    *   `each word in a language can take multiple Part-of-Speech (POS) categories...`: **True, but incomplete.** This is *part* of the reason for the ambiguity ("duck" as noun vs. verb), but the core lesson is that context is the key to solving it. The correct answer is a broader, more encompassing conclusion.

---

### **Question 14**
**Question:** Considering the taxonomy of speech acts defined by Bach and Harnish (in 1979), when someone advises/asks/orders/requests somebody, they are performing which type of speech act?
**Correct Answer:** a directive: attempt by speaker to get addressee to do something

*   **Analysis:**
    *   **Correct:** A **directive** is a speech act that aims to make the hearer perform some action. Advising, asking, ordering, and requesting are all classic examples of trying to direct someone's behavior.
    *   `an acknowledgment`: **Incorrect.** This expresses feelings about the hearer (e.g., apologizing, congratulating).
    *   `a commissive`: **Incorrect.** This commits the *speaker* to a future action (e.g., promising, offering).
    *   `a constative`: **Incorrect.** This commits the speaker to the truth of a proposition (e.g., stating, claiming).

---

### **Question 15**
**Question:** Regular expressions provide powerful language for writing rules to extract content from text documents, but have various limitations. Which of the following would NOT be considered a limitation of regular-expression based text extraction:
**Correct Answer:** the simplicity of the approach

*   **Analysis:** The question asks what is NOT a limitation.
    *   **Correct:** The simplicity of writing and using regular expressions for basic pattern matching is one of their main **strengths**, not a limitation.
    *   `false positives, due to insufficiency of syntactical structure...`: **Limitation.** Regex doesn't understand grammar, so a pattern might match in unintended contexts.
    *   `the difficulty of writing extraction rules by hand`: **Limitation.** Writing complex, robust regex can be very difficult and error-prone.
    *   `the difficulty/inability to integrate knowledge of context...`: **Limitation.** Regex is stateless and knows nothing beyond the characters it is currently matching.
    *   `false negatives, due to lack of generality of the rule used`: **Limitation.** A rigid regex rule may fail to match valid variations of a pattern.

---

### **Question 16**
**Question:** In Multi-task learning, we fine-tune a Language model to perform many different tasks (with different prompts) at the same time. The aim/benefit of doing this is to get the model to:
**Correct Answer:** ALL the other answers are correct

*   **Analysis:** Multi-task learning aims to improve a model's overall capability and generalization by training it on multiple related tasks.
    *   `become so good at general-purpose question answering...`: **Benefit.** Training on diverse tasks can lead to better generalization on new, unseen tasks.
    *   `learn to do many tasks contemporaneously, so we don't need to deploy and maintain many task-specific models...`: **Benefit.** This is a major practical advantage, simplifying model deployment and maintenance.
    *   `generalise knowledge across the different tasks and thus to perform better on any particular task`: **Benefit.** This is the core theoretical advantage. The model learns more robust, underlying representations by seeing how they apply across different contexts, which can boost performance on each individual task.
    *   Since all listed options are valid benefits of multi-task learning, this is the correct choice.

---

### **Question 17**
**Question:** In a web search engine, rank learning algorithms combine various different signals together to rank webpages for a query. Which of the following signals might provide for useful features to the rank learner?
**Correct Answer:** ALL of the other options are correct

*   **Analysis:** A learning-to-rank model for search uses a vast array of features to determine relevance.
    *   `similarity between the text of incoming hyperlinks and the query`: **Useful Feature.** This is the core idea behind Google's original PageRank algorithm (anchor text).
    *   `importance/reliability of the portal hosting the content`: **Useful Feature.** Domain authority is a very strong signal.
    *   `similarity between the webpage content and the query`: **Useful Feature.** A basic textual relevance signal (e.g., TF-IDF, BM25).
    *   `similarity between the webpage title and the query`: **Useful Feature.** The title is a heavily weighted field.
    *   `number of hyperlinks from other webpages...`: **Useful Feature.** A measure of popularity/authority.
    *   `distance between geographic locations...`: **Useful Feature.** Crucial for local search (e.g., "library near me").
    *   All these are classic and important features in web search ranking.

---

### **Question 18**
**Question:** Which of the following statements about the Logistic Regression (LR) classifier is true?
**Correct Answer:** LR produces well-calibrated probability estimates

*   **Analysis:**
    *   **Correct:** One of the key strengths of Logistic Regression (when properly trained) is that its output can be directly interpreted as a probability, and these probabilities are generally well-calibrated (i.e., if it predicts a class with 80% probability, it's correct about 80% of the time).
    *   `LR produces curved decision boundaries`: **Incorrect.** LR is a linear model; it produces a linear (hyperplane) decision boundary.
    *   `LR makes use of the Hinge loss`: **Incorrect.** Hinge loss is used by Support Vector Machines (SVMs). LR uses Log Loss (or cross-entropy loss).
    *   `LR is rarely used in practical settings`: **Incorrect.** It's extremely common as a fast, simple, and interpretable baseline for classification tasks.
    *   `LR does not scale well to a large numbers of features`: **Incorrect.** LR scales very well, especially with sparse features, and is often used in situations with millions of features (e.g., text classification with a bag-of-words model).

---

### **Question 19**
**Question:** The vector: `[0,0,0,0,1,0,5,0,0,0,2,3,0,0,0,0,5,...]` could be:
**Correct Answer:** a bag-of-words vector representing a document

*   **Analysis:**
    *   **Correct:** A **bag-of-words (BoW)** vector has a dimension for each word in the vocabulary. Each element in the vector contains the count (or frequency, like TF-IDF) of that word in the document. The presence of integers greater than 1 (e.g., 5, 2, 3) indicates word counts, which is characteristic of BoW. The vector is also sparse (mostly zeros), which is also typical.
    *   `a one-hot vector representing the word "apple"`: **Incorrect.** A one-hot vector has exactly one element with the value 1 and all others are 0.
    *   `a topic vector representing a document`: **Incorrect.** A topic vector (e.g., from LDA) would have dense, non-integer, fractional values representing the proportion of each topic in the document.
    *   `an embedding vector representing a word`: **Incorrect.** An embedding vector (e.g., from Word2Vec) is a dense vector with many non-zero, floating-point values.

---

### **Question 20**
**Question:** Which of the following statements about current open-source LLM technology (in 2024) is NOT true?
**Correct Answer:** Current open-source LLMs contain thousands of transformer layers

*   **Analysis:**
    *   **Correct (as the false statement):** State-of-the-art models, even massive ones, have dozens or perhaps low hundreds of transformer layers, not thousands. For example, Llama 2 70B has 80 layers. Having thousands of layers would be computationally intractable to train and run.
    *   `have vocabulary sizes in the tens of thousands`: **True.** Vocabularies are typically between 32k and 128k.
    *   `have billions of parameters`: **True.** Open-source models range from ~1B to >100B parameters (e.g., Mistral 7B, Llama 70B, Falcon 180B).
    *   `have been trained on trillions of tokens of text`: **True.** Large models are trained on web-scale datasets containing trillions of tokens.
    *   `use embeddings with thousands of dimensions`: **True.** Embedding sizes are often in the thousands (e.g., 4096 for Llama 2).

---

### **Question 21**
**Question:** When generating text from a language model with top-k sampling, setting the value of k to the size of the vocabulary would be equivalent to performing:
**Correct Answer:** random sampling

*   **Analysis:**
    *   **Correct:** Top-k sampling works by (1) selecting the `k` most likely next tokens and (2) sampling from that reduced set after renormalizing their probabilities. If you set `k` to be the entire vocabulary size, you are simply selecting *all* possible tokens. Sampling from the full, un-truncated probability distribution is the definition of **random sampling** (or ancestral sampling).
    *   `beam search`: **Incorrect.** Beam search is a deterministic (or near-deterministic) search algorithm that keeps track of the `b` most likely sequences at each step; it is not a sampling method.
    *   `greedy sampling`: **Incorrect.** This would be equivalent to setting `k=1`, where you always pick the single most likely token.
    *   `t-SNE dimensionality reduction`: **Incorrect.** t-SNE is a visualization technique, completely unrelated to text generation.

---

### **Question 22**
**Question:** A statistical language model computes:
**Correct Answer:** a probability distribution over sequences of words

*   **Analysis:**
    *   **Correct:** This is the fundamental definition of a language model. Its core task is to assign a probability `P(w1, w2, ..., wn)` to any sequence of words.
    *   `a probability distribution over emotions...`: **Incorrect.** This is the task of a sentiment analysis or emotion detection classifier.
    *   `statistics like the average document length...`: **Incorrect.** These are corpus statistics, which might be used to build a model, but are not what the model itself computes.
    *   `a probability distribution over grammatical classes...`: **Incorrect.** This is the task of a Part-of-Speech (POS) tagger.
    *   `a probability distribution over languages...`: **Incorrect.** This is the task of a language identification model.

---

### **Question 23**
**Question:** Given the following conditional probabilities for a trigram language model... What would be the probability of the sequence "I like chocolate ice cream" be?
**Correct Answer:** 1/128

*   **Analysis:** The probability of a sequence using an n-gram model is the product of the conditional probabilities of each word given its context.
    *   `P("I like chocolate ice cream") = P(I) * P(like | I) * P(chocolate | I, like) * P(ice | like, chocolate) * P(cream | chocolate, ice)`
    *   The problem provides the necessary probabilities, sometimes using a shorter context (a process called backoff, which is implicitly used here):
        *   `P(I)` = 1/4 (given as a unigram)
        *   `P(like | I)` = 1/2 (given as a bigram)
        *   `P(chocolate | I, like)` = 1/4 (given as a trigram)
        *   `P(ice | like, chocolate)` = 1/4 (given as a trigram)
        *   `P(cream | chocolate, ice)` = 1 (given as a trigram)
    *   **Calculation:** Multiply the probabilities: `(1/4) * (1/2) * (1/4) * (1/4) * 1 = 1 / (4 * 2 * 4 * 4) = 1 / 128`.

---

### **Question 24**
**Question:** Which, if any, of the following techniques is NOT used to produce a spectrogram for analysing audio signals to understand spoken language:
**Correct Answer:** k-Means clustering

*   **Analysis:** A spectrogram is created by taking short, overlapping segments of an audio signal and computing the frequency content of each segment.
    *   **Correct (as the unused technique):** K-Means is a clustering algorithm. It is used to group data points, not to analyze the frequency content of a signal. It has no role in the standard Short-Time Fourier Transform (STFT) process.
    *   `Pre-emphasis filtering`: **Used.** This is a common first step to boost high-frequency components of the signal.
    *   `Hamming windows on overlapping time-segments`: **Used.** A windowing function (like Hamming) is applied to each segment to reduce spectral leakage before the Fourier transform.
    *   `Fast Fourier Transform (FFT)`: **Used.** This is the core algorithm used to efficiently compute the frequency spectrum of each windowed segment.

---

### **Question 25**
**Question:** Byte-pair encoding:
**Correct Answer:** agglomerates common consecutive characters together to form sub-word tokens

*   **Analysis:**
    *   **Correct:** This is the definition of the BPE algorithm. It starts with a vocabulary of individual characters and iteratively merges the most frequent adjacent pair of tokens to create a new, longer sub-word token.
    *   `is a way of quantising the parameters...`: **Incorrect.** This describes quantization, a model compression technique.
    *   `represents each word as a byte-pair vector...`: **Incorrect.** BPE creates a vocabulary of tokens; it doesn't create vectors itself.
    *   `encodes text as byte-pairs: 2 bytes for each character...`: **Incorrect.** This is a misinterpretation of the name. BPE operates on frequent pairs of *tokens*, not on a fixed 2-byte-per-character encoding scheme.

---

### **Question 26**
**Question:** When generating text from a language model, which of the following techniques will likely require the most computational resources and thus be slowest to generate text?
**Correct Answer:** beam search

*   **Analysis:**
    *   **Correct:** **Beam search** is the most computationally expensive. At each step, instead of generating just one token, it explores `b` (the beam width) possible next tokens for *each* of the `b` sequences it is currently tracking. It then scores all `b*b` new candidate sequences and keeps the top `b`. This requires `b` times more computation at each step compared to the other sampling methods.
    *   `top-k sampling`, `random sampling`, `greedy sampling`: **Incorrect.** These are all "single-path" methods. At each step, they perform one forward pass, get the probabilities, and select *one* next token. They are all much faster than beam search.

---

### **Question 27**
**Question:** "Paris Hilton was photographed leaving the Paris Hilton" is an example of a sentence containing:
**Correct Answer:** a person entity and an organisation entity

*   **Analysis:** This is a classic Named Entity Recognition (NER) ambiguity problem.
    *   **Correct:**
        *   The first "Paris Hilton" refers to the **PERSON**.
        *   The second "Paris Hilton" refers to a hotel, which is an **ORGANISATION** (or sometimes classified as a LOCATION/FACILITY).
    *   The other options are incorrect because they fail to capture both distinct entity types. `a geo-political entity referenced twice` would be incorrect as neither is a city in this context.

---

### **Question 28**
**Question:** Which statement about Word2Vec is true?
**Correct Answer:** Word2Vec is trained to predict whether a missing word fits given a particular surrounding context

*   **Analysis:** This describes the core training objective of Word2Vec (specifically, the Continuous Bag-of-Words, CBOW, or Skip-gram models).
    *   **Correct:** The goal of the training "game" is to predict a context word given a center word (Skip-gram) or a center word given its context (CBOW). The word embeddings are the learned parameters of the shallow neural network that gets good at this game. The prompt incorrectly describes the Negative Sampling objective (predicting if a word *fits* vs. just predicting the word), but it's the closest and most conceptually correct description among the options.
    *   `learns dense vector representations for documents`: **Incorrect.** Word2Vec learns representations for *words*. Document vectors can be created by averaging the word vectors, but that's a downstream task.
    *   `learns sparse vector representations for words`: **Incorrect.** Word2Vec learns *dense* vectors.
    *   `is based on a recurrent neural network architecture`: **Incorrect.** Word2Vec uses a simple, shallow, feed-forward neural network. RNNs are used for sequential modeling.

---

### **Question 29**
**Question:** Which of the following techniques used in NLP is the most recent and considered state-of-the-art?
**Correct Answer:** Transformer models

*   **Analysis:** This question is about the historical progression of dominant NLP architectures.
    *   **Correct:** **Transformer models** (introduced in 2017) are the foundation for virtually all modern state-of-the-art models (BERT, GPT, T5, etc.) and have superseded previous architectures for most tasks.
    *   **Older -> Newest:**
        1.  Hidden Markov models (HMMs) & Context-free grammars (CFGs) (pre-2000s)
        2.  Conditional Random Fields (CRFs) & Support Vector Machines (SVMs) (2000s)
        3.  Recurrent Neural Networks (RNNs/LSTMs) (early-mid 2010s)
        4.  **Transformers** (late 2010s - present)

---

### **Question 30**
**Question:** The formula shown is sometimes used to compute similarity within the attention mechanism... Which (if any) of the following statements about the formula is NOT correct?
**Formula:** `similarity(h_i-1, e_j) = (h_i-1 ⋅ e_j) / √d`
**Correct Answer:** ALL of the other statements are CORRECT

*   **Analysis:** The formula shown is for **scaled dot-product attention**.
    *   `The variable "d" denotes the number of dimensions of the embedding vector`: **Correct.** The scaling factor `√d` is the square root of the vector dimensionality.
    *   `The variable "h" denotes the previous state of the decoder`: **Correct.** `h_i-1` is typically the decoder's hidden state from the previous step, which acts as the "query".
    *   `The formula computes multiplicative (rather than additive) attention weights`: **Correct.** This is multiplicative attention because it's based on a dot product. Additive attention would involve a feed-forward network and a `tanh` activation.
    *   `The denominator normalizes the dot product to have a standard deviation of 1`: **Correct.** This is the theoretical reason for the scaling factor. It prevents the dot products from growing too large for high-dimensional vectors, which would push the softmax function into regions with very small gradients.
    *   `The variable "e" denotes an embedding vector for a certain position produced by the encoder`: **Correct.** `e_j` represents the encoder outputs (the "keys" and "values").
    *   Since all the individual statements are correct descriptions of the formula's components and purpose, the correct choice is that they are all correct.

---
... and so on for the remaining questions. The full Markdown file would continue this pattern for all 86 questions. I'll continue with the remaining questions below.

---

### **Question 31**
**Question:** Machine translation is an example of what type of problem?
**Correct Answer:** sequence-to-sequence problem

*   **Analysis:**
    *   **Correct:** Machine translation takes a sequence of words in one language (the source) and outputs a sequence of words in another language (the target). This is the canonical example of a **sequence-to-sequence (seq2seq)** task.
    *   `regression problem`: **Incorrect.** Regression predicts a continuous value.
    *   `clustering problem`: **Incorrect.** Clustering is an unsupervised task of grouping similar items.
    *   `masked-language modelling problem`: **Incorrect.** This is a pre-training objective (like for BERT), not a downstream task itself.
    *   `classification problem`: **Incorrect.** Classification assigns a single label from a fixed set.
    *   `sequence-labeling problem`: **Incorrect.** Sequence labeling assigns a label to *each* token in the input sequence (e.g., POS tagging). The output length is tied to the input length, which is not the case for translation.

---

### **Question 32**
**Question:** Imagine you are building a Naive Bayes classifier for detecting spam emails. You have a collection of 2,000 spam emails and 8,000 not-spam emails... what is the PRIOR probability of seeing a spam email?
**Correct Answer:** 20%

*   **Analysis:**
    *   **Prior Probability:** The prior probability of a class is simply its frequency in the training dataset, before looking at any features (words).
    *   **Total Emails:** 2,000 (spam) + 8,000 (not-spam) = 10,000 total emails.
    *   **Spam Emails:** 2,000.
    *   **Calculation:** `P(spam) = (Number of Spam Emails) / (Total Number of Emails) = 2,000 / 10,000 = 0.20`.
    *   This is equal to **20%**.

---

### **Question 33**
**Question:** Which traditional (lexical) retrieval function would be most robust to a spammer who tries to push their web page up... by adding many occurrences of the query term...
**Correct Answer:** BM25

*   **Analysis:** This is a question about "term frequency saturation."
    *   **Correct:** **BM25** has a built-in term frequency saturation component. After a term appears a few times in a document, adding more occurrences provides diminishing returns in the relevance score. This makes it much more robust to "keyword stuffing" than standard TF-IDF.
    *   `TF-IDF`: **Incorrect.** Standard TF-IDF's TF component is often linear or logarithmic. A linear TF is highly vulnerable to keyword stuffing, as every additional occurrence increases the score by the same amount.
    *   `IDF`: **Incorrect.** IDF is a property of the term across the whole collection, not the document. It's a component of TF-IDF and BM25, not a retrieval function on its own.
    *   `Cosine`: **Incorrect.** Cosine similarity on its own (with TF vectors) would also be susceptible to keyword stuffing.
    *   `Self-attention`: **Incorrect.** This is a component of a neural network, not a traditional lexical retrieval function.

---

### **Question 34**
**Question:** Which statement about GPT (GPT-2, GPT-3, etc) models is NOT true?
**Correct Answer:** GPT is a bidirectional text encoder.

*   **Analysis:**
    *   **Correct (as the false statement):** This is fundamentally incorrect. GPT models are **unidirectional** (autoregressive). They read text from left-to-right, and when predicting a token, they can only see the tokens that came before it. This is what makes them "Generative". Bidirectionality is the hallmark of models like BERT.
    *   `GPT stands for Generative Pre-trained Transformer`: **True.**
    *   `GPT models are preferred over BERT models for text generation tasks`: **True.** GPT's architecture is explicitly designed for generation, while BERT's is designed for understanding tasks where the whole sentence is available.
    *   `ChatGPT is based on recent version of the GPT model`: **True.** It's based on the GPT-3.5 and GPT-4 series of models.

---

### **Question 35**
**Question:** While we don't know the details of OpenAI's GPT-3.5 and GPT-4 models... it is however highly likely given their names that:
**Selected Answer (marked incorrect):** the models make use of a encoder-decoder architecture
**Likely Correct Answer:** the models make use of an autoregressive architecture

*   **Analysis:** The question asks for the most likely truth.
    *   `the models make use of an autoregressive architecture`: **Highly Likely/True.** The entire GPT lineage is built on the autoregressive, decoder-only transformer architecture. It is the defining characteristic.
    *   `the models make use of a encoder-decoder architecture`: **Highly Unlikely/False.** This is the architecture of models like T5 and BART, not GPT. The user's choice was correctly marked as incorrect.
    *   `the models use a bidirectional architecture`: **False.** As explained before, they are unidirectional.
    *   The other options use made-up terms ("Generic Parallel Tokenisers", "Guided Parental Teaching") or factually incorrect numbers for parameters and size, making them easy to dismiss.

---

### **Question 36**
**Question:** The Mel Spectrogram has:
**Correct Answer:** a linear time axis, logarithmic frequency axis and logarithmic amplitude scale

*   **Analysis:**
    *   **Time Axis:** The x-axis represents time, which progresses linearly.
    *   **Frequency Axis:** The y-axis represents frequency. In a Mel spectrogram, the frequency scale is converted to the **Mel scale**, which is **logarithmic** for human pitch perception (humans are more sensitive to changes in lower frequencies).
    *   **Amplitude/Intensity:** The color/intensity of each point represents the amplitude (energy) of that frequency at that time. This is typically measured in decibels (dB), which is a **logarithmic** scale.

---

### **Question 37**
**Question:** Consider the regular expression: `"\d{1,2}-(Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)-\d{2,4}"`. Which, if any, of the following strings would the expression match?
**Correct Answer:** 3-Mar-123

*   **Analysis:** Let's break down the regex:
    *   `\d{1,2}`: Matches one or two digits.
    *   `-`: Matches a literal hyphen.
    *   `(Jan|...|Dec)`: Matches one of the three-letter month abbreviations.
    *   `-`: Matches a literal hyphen.
    *   `\d{2,4}`: Matches two, three, or four digits.

*   Let's test the options:
    *   `21 May 2025`: **No match.** Uses spaces instead of hyphens, and the full month name "May" instead of the abbreviation "May".
    *   `2023-Apr-25`: **No match.** The first part "2023" has four digits, but the regex `\d{1,2}` only allows one or two.
    *   `1:Jan:0000`: **No match.** Uses colons instead of hyphens.
    *   **`3-Mar-123`**: **Match!** "3" matches `\d{1,2}`, "-" matches "-", "Mar" matches the month group, "-" matches "-", and "123" matches `\d{2,4}`.
    *   `29-Fe-2000`: **No match.** "Fe" is not one of the valid month abbreviations.
    *   `01--Jun--2012`: **No match.** Uses double hyphens.

---

### **Question 38**
**Question:** In terms of speech acts (and discourse analysis), when someone repeats back to the speaker part of what they have just said, what is usually the purpose of doing this?
**Correct Answer:** to acknowledge understanding of the conveyed information and establish a common ground

*   **Analysis:** This is a key concept in discourse and dialog systems.
    *   **Correct:** Repeating or paraphrasing (called "grounding") is a conversational mechanism to confirm that the listener has heard and understood correctly. It establishes a shared understanding ("common ground") upon which the conversation can be built.
    *   `to be polite and sound attentive`: **Partially true, but not the primary purpose.** It is a polite action, but the core function is about ensuring mutual understanding.
    *   `to keep the conversation going...`: **Incorrect.** It's not just about filling silence; it has a specific communicative function.
    *   `to sound smart and coax more information...`: **Incorrect.** This describes a manipulative interrogation technique, not the typical collaborative purpose of grounding in normal conversation.

---

### **Question 39**
**Question:** When evaluating dialog produced by a chatbot, ideally we would rate performance based on:
**Correct Answer:** ALL of the other responses are valid

*   **Analysis:** Evaluating conversational AI is a multi-faceted problem with no single perfect metric.
    *   `how engaging the dialog is`: **Valid Metric.** User engagement is a key goal.
    *   `how interesting the dialog is`: **Valid Metric.** Similar to engagement.
    *   `how infrequently the chatbot repeats itself`: **Valid Metric.** Repetitiveness is a common failure mode.
    *   `how correct and consistent the responses are`: **Valid Metric.** Factual accuracy and consistency are crucial for task-oriented and informational bots.
    *   `how inquisitive and sympathetic the chatbot seems`: **Valid Metric.** These qualities (empathy, proactiveness) contribute to a better user experience.
    *   Since all are desirable qualities and valid evaluation criteria, this is the correct answer.

---

### **Question 40**
**Question:** Which statement about the T5 (Text-To-Text Transfer Transformer) model is NOT true?
**Correct Answer:** T5 is a distilled (smaller) version of BERT

*   **Analysis:**
    *   **Correct (as the false statement):** T5 is not a distilled version of BERT. It is a completely different architecture and pre-training paradigm. DistilBERT is the distilled version of BERT. T5 models are often very large themselves (e.g., T5-11B).
    *   `T5 makes use of bidirectional encoder and causal decoder`: **True.** T5 uses the standard encoder-decoder Transformer architecture. The encoder is bidirectional (like BERT) and the decoder is causal/autoregressive (like GPT).
    *   `T5 makes use of a relative positional encoding`: **True.** This was one of the details of the T5 paper, moving away from absolute positional encodings.
    *   `T5 is particularly useful for translation tasks`: **True.** Its encoder-decoder structure is naturally suited for seq2seq tasks like translation and summarization.
    *   `T5 has an encoder-decoder architecture...`: **True.** This is its defining feature.

---

... I will now continue with the rest of the questions.

---

### **Question 41**
**Question:** You are building a document search system and train a BERT model to classify whether or not a document is relevant to a particular user query. How might you reduce the computational burden when deploying this system without degrading greatly the performance of it?
**Correct Answer:** By using a lexical or semantic search engine to first find a set of potentially relevant documents

*   **Analysis:** This describes a **two-stage re-ranking architecture**, which is a standard industry practice.
    *   **Correct:** BERT is very powerful but also very slow. Running it on millions of documents for every query is impossible. The solution is to use a fast, "first-pass" retrieval system (like BM25 or a vector search) to retrieve a small set of promising candidates (e.g., top 100). Then, the powerful but slow BERT "re-ranker" is applied only to this small set to produce the final, high-quality ranking. This balances efficiency and quality.
    *   `By using GPT2 rather than BERT...`: **Incorrect.** GPT-2 is generally less effective for classification tasks than BERT and isn't necessarily much faster in a comparable size.
    *   `By replacing the BERT model with a linear classifier`: **Incorrect.** This would greatly degrade performance, as a linear classifier is much less powerful than BERT.
    *   `By training the BERT model on sentiment analysis...`: **Incorrect.** Training on the wrong task (sentiment vs. relevance) would destroy performance.

---

### **Question 42**
**Question:** Reinforcement Learning from Human Feedback is usually used to do what?
**Correct Answer:** turn a pre-trained LLM into a Chatbot

*   **Analysis:**
    *   **Correct:** RLHF is the key process that was used to create models like InstructGPT and ChatGPT. It takes a base, pre-trained LLM (which is good at completing text but not necessarily at following instructions) and fine-tunes it to be a helpful and harmless instruction-following assistant (a chatbot). The "human feedback" (ranking different model responses) is used to train a reward model, which then guides the fine-tuning process using reinforcement learning.
    *   `evaluate the performance of a pre-trained LLM`: **Incorrect.** Evaluation is part of the process, but the ultimate goal is to *change* the model's behavior, not just measure it.
    *   `train an LLM to understand customer feedback`: **Incorrect.** This is too specific. RLHF is a general technique, not limited to customer feedback.
    *   `reinforce stereotypes held by humans...`: **Incorrect.** The goal of RLHF is often the opposite: to align the model with desired values like harmlessness, which includes *reducing* the output of stereotypes.

---

### **Question 43**
**Question:** GloVE embeddings
**Correct Answer:** represent words as numerical vectors

*   **Analysis:**
    *   **Correct:** GloVE (Global Vectors for Word Representation) is an algorithm, like Word2Vec, whose purpose is to learn a dense **numerical vector** for each word in a vocabulary.
    *   `represent images...`, `represent topics...`, `represent document...`: **Incorrect.** GloVE is specifically for words.
    *   `provide a low dimensional approximation of a high dimensional representation`: **Incorrect.** This is the definition of dimensionality reduction techniques like PCA or t-SNE, not word embedding models.
    *   `provide a sparse representation`: **Incorrect.** GloVE provides *dense* vectors.

---

### **Question 44**
**Question:** The main reason for performing stemming before building a text classifier is to:
**Correct Answer:** reduce the feature set while increasing the number of examples of each token

*   **Analysis:**
    *   **Correct:** Stemming maps multiple variations of a word (e.g., "run", "running", "ran") to a single stem (e.g., "run"). This has two effects:
        1.  **Reduces the feature set:** The vocabulary size shrinks because "running" and "ran" are no longer treated as separate features from "run".
        2.  **Increases examples:** All occurrences of the different variations now count towards the single stem, providing more statistical evidence for that conceptual token. This helps the model generalize better, especially with smaller datasets.
    *   `correct misspellings...`: **Incorrect.** Stemming does not correct spelling.
    *   `to lowercase the text`: **Incorrect.** This is a separate preprocessing step called case-folding.
    *   `concentrate representation around STEM... terms`: **Incorrect.** Another humorous distractor.
    *   `to split overly long words...`: **Incorrect.** This describes tokenization or sub-word splitting, not stemming.

---

### **Question 45**
**Question:** Building spoken interface agents is harder than building chatbots because:
**Correct Answer:** ALL of the other reasons are valid

*   **Analysis:** Spoken interfaces add layers of complexity on top of text-based chatbots.
    *   `system needs to place emphasis on certain parts...`: **Valid Reason.** Prosody (stress, intonation) is crucial for conveying meaning in speech and is hard to generate naturally.
    *   `system needs to detect when the user is interrupting...`: **Valid Reason.** Barge-in detection is a difficult but necessary feature for natural turn-taking.
    *   `system needs to be able to detect that the user has finished talking...`: **Valid Reason.** Voice Activity Detection (VAD) is a non-trivial problem.
    *   `system must be robust to speech to text errors...`: **Valid Reason.** ASR is never perfect, and the downstream NLU model must be able to handle noisy, incorrect transcriptions.
    *   `system needs to respond immediately...`: **Valid Reason.** Latency is much more noticeable and frustrating in spoken conversation than in text chat.
    *   All these are significant challenges unique to voice interfaces.

---

### **Question 46**
**Question:** If we apply 4-bit quantisation when loading a Mistral-7B LLM on a GPU, approximately how much video RAM would be needed to store the parameters of the model?
**Correct Answer:** 3.5 GB

*   **Analysis:**
    1.  **Model Size:** Mistral-7B has 7 billion (7 x 10^9) parameters.
    2.  **Quantization:** 4-bit quantization means each parameter is stored using 4 bits.
    3.  **Bits to Bytes:** There are 8 bits in 1 byte. So, 4 bits is equal to 0.5 bytes.
    4.  **Total Size in Bytes:** `Total Bytes = (Number of Parameters) * (Bytes per Parameter) = (7 * 10^9) * 0.5 = 3.5 * 10^9` bytes.
    5.  **Bytes to Gigabytes:** There are 10^9 bytes in a gigabyte (GB).
    6.  **Final Size:** `3.5 * 10^9 bytes = 3.5 GB`.
    *   Note: A small amount of additional memory is needed for overhead, but 3.5 GB is the approximate size for the weights themselves.

---

### **Question 47**
**Question:** You train a multilingual BERT based classifier and find that the model is performing particularly poorly, with an accuracy that is far below the level of the linear classifier. What might you do to try to improve the performance of the model?
**Correct Answer:** ALL of the other answers are valid

*   **Analysis:** When a complex model like BERT underperforms a simple baseline, it often indicates a problem with training or model choice. All the options are reasonable debugging steps.
    *   `try increasing the batch size...`: **Valid Step.** A larger batch size can lead to more stable gradient estimates and sometimes better convergence.
    *   `train it for another epoch...`: **Valid Step.** The model might simply be undertrained.
    *   `check whether a monolingual BERT model exists for your language...`: **Valid Step.** A specialized monolingual model will almost always outperform a general multilingual model on that specific language. This is a crucial check.
    *   `re-start the training with a lower setting of the learning rate`: **Valid Step.** The current learning rate might be too high, causing the training to be unstable or diverge.
    *   `try to train a larger version of the BERT model`: **Valid Step.** A larger model has more capacity and might be able to capture the patterns better (assuming the issue isn't unstable training).

---

### **Question 48**
**Question:** The act of attributing human emotions and intentions to a computer program (such as a chatbot) is referred to as:
**Correct Answer:** anthropomorphism

*   **Analysis:**
    *   **Correct:** **Anthropomorphism** is the a-scription of human traits, emotions, or intentions to non-human entities. This is precisely what happens when we say a chatbot is "happy" or "wants" to help us.
    *   `chatGPT-ification`: **Incorrect.** A made-up term.
    *   `artificial general intelligence (AGI)`: **Incorrect.** AGI is a hypothetical type of AI that possesses human-like intelligence across a wide range of tasks; it's not the act of attributing these traits.
    *   `hallucination`: **Incorrect.** This refers to a model generating factually incorrect or nonsensical information.
    *   `lexicalisation`, `prosody`, `semantics`, `morphology`, `dependency parsing`: **Incorrect.** These are all specific linguistic or NLP concepts unrelated to the psychological act of attribution.

---

### **Question 49**
**Question:** Consider the following normalised tf-idf vectors... And the 4-term query... What would be the order of the documents if the cosine similarity is used to rank them?
**Correct Answer:** C, D, A, E, B

*   **Analysis:**
    1.  **Cosine Similarity:** Since all document vectors `A-E` are normalized, the ranking will be directly proportional to the dot product of the query vector `q` with each document vector.
    2.  **Query Vector `q`:** "awesome blueberry muffin recipe". The provided vector `q = [ 0, 0.25, 0.25, 0.25, 0.25, ...]` indicates that these four words correspond to indices 1, 2, 3, and 4 of the vectors (assuming 0-based indexing for the list).
    3.  **Calculate Dot Products `q · d`:** We can ignore the constant `0.25` from the query for ranking purposes and just sum the components of each document vector at indices 1, 2, 3, and 4.
        *   `Score(A)`: `0.0 + 0.1 + 0.1 + 0.0 = 0.2`
        *   `Score(B)`: `0.0 + 0.0 + 0.0 + 0.0 = 0.0`
        *   `Score(C)`: `0.0 + 0.1 + 0.0 + 0.3 = 0.4`
        *   `Score(D)`: `0.1 + 0.0 + 0.2 + 0.0 = 0.3`
        *   `Score(E)`: `0.0 + 0.1 + 0.0 + 0.0 = 0.1`
    4.  **Rank by Score (Highest to Lowest):**
        *   1st: C (0.4)
        *   2nd: D (0.3)
        *   3rd: A (0.2)
        *   4th: E (0.1)
        *   5th: B (0.0)
    5.  The final order is **C, D, A, E, B**.

---

### **Question 50**
**Question:** Text normalisation is needed for a text-to-speech system in order to:
**Correct Answer:** expand numeric values like "123" into word form: "one hundred and twenty three"

*   **Analysis:** Text normalization in a TTS pipeline is the process of converting text from its written form into a "speakable" form.
    *   **Correct:** A TTS system doesn't know how to pronounce "123". It must be expanded into words ("one hundred and twenty three") for the model to generate the correct sounds. This also applies to abbreviations ("Dr." -> "doctor"), currency ("$5" -> "five dollars"), and other non-standard words.
    *   `scale the length of the sentence embedding...`: **Incorrect.** This is related to model architecture, not text normalization.
    *   `scale the amplitude of the output audio signal...`: **Incorrect.** This is audio signal processing, which happens *after* the text has been converted to an audio waveform.

---
This concludes the first 50 questions. The remaining will follow.
---

### **Question 51**
**Question:** Which statement about the limitations of Ngram language models is NOT correct:
**Correct Answer:** the expected number of occurrences of an n-gram decreases linearly with the length of the ngram

*   **Analysis:** The question asks for the INCORRECT statement about limitations.
    *   **Correct (as the false statement):** The number of occurrences of an n-gram decreases **exponentially**, not linearly, with `n`. The probability of a specific 5-gram is far, far lower than a 4-gram. This rapid decrease is what leads to the sparsity problem.
    *   `storing counts for high-order ngrams requires massive amounts of memory`: **True Limitation.** The number of possible n-grams grows exponentially with `n`.
    *   `the training corpus is never big enough to estimate high-order ngrams well`: **True Limitation.** This is the data sparsity problem. Most higher-order n-grams will never appear in the training data.
    *   `predictive performance of an ngram model depends greatly on the maximum ngram length...`: **True Limitation.** There's a trade-off. Longer n-grams capture more context but suffer more from sparsity. Shorter n-grams are more robust but less context-aware.

---

### **Question 52**
**Question:** The task of determining who or what is being referred to by a pronoun in a sentence is called:
**Correct Answer:** Co-reference resolution

*   **Analysis:**
    *   **Correct:** **Co-reference resolution** is the task of finding all expressions in a text that refer to the same real-world entity. This includes linking pronouns (e.g., "he", "it", "she") to their antecedents (e.g., "John", "the car").
    *   `De-pronounification`: **Incorrect.** Not a standard NLP term.
    *   `Part-of-speech tagging`: **Incorrect.** This labels words with their grammatical category (noun, verb, etc.).
    *   `Named entity recognition`: **Incorrect.** This finds and classifies named entities (Person, Organization, Location), but doesn't link them together.

---

### **Question 53**
**Question:** The process of aligning words to a common reference dictionary (e.g. by removing punctuation), so as to ensure consistent spelling/formatting throughout the corpus is referred to as:
**Correct Answer:** Word normalisation

*   **Analysis:**
    *   **Correct:** **Word normalization** (or text normalization) is the general term for a set of preprocessing tasks that aim to put all text into a consistent format. This includes case-folding, removing punctuation, and handling numbers and symbols. Stemming and lemmatization are specific types of normalization.
    *   `Word embedding`: **Incorrect.** This is the process of creating vector representations for words.
    *   `Word de-punctuation-ification`: **Incorrect.** A made-up, overly specific term.
    *   `Word cleanup`, `Word encoding`: **Incorrect.** These are too generic. "Normalization" is the precise technical term.

---

### **Question 54**
**Question:** Which of the following prompts to a language model would be considered an example of one-shot learning?
**Correct Answer:** "english: awesome exam, italiano: esame fantastico, english: mark rules, italiano: "

*   **Analysis:**
    *   **Zero-shot:** The model is given a task description with no examples. E.g., "Translate 'awesome exam' to French."
    *   **One-shot:** The model is given exactly **one** complete example of the task before being prompted to perform it.
    *   **Few-shot:** The model is given a few (2 or more) examples.

*   **Testing the options:**
    *   `"1 + 1 = 2, 1 + 2 = 3,..."`: **Few-shot.** Multiple examples are provided.
    *   **`"english: awesome exam, italiano: esame fantastico, english: mark rules, italiano: "`**: **One-shot.** It provides one full example (`awesome exam` -> `esame fantastico`) and then gives the new input (`mark rules`) to be completed.
    *   `"How do you say 'awesome exam' in French?"`: **Zero-shot.** This is a direct instruction with no examples.
    *   `"apple => fruit, broccoli => vegetable,..."`: **Few-shot.** Multiple examples.

---

### **Question 55**
**Question:** In text-to-speech systems, certain words like "bass" can be problematic. Why?
**Correct Answer:** because it is a homograph, with two possible meanings with different pronunciations

*   **Analysis:**
    *   **Correct:** "bass" is a **homograph** - a word with the same spelling but different meanings and pronunciations. It can be pronounced /beɪs/ (the fish) or /bæs/ (the musical instrument/sound). A TTS system needs to use context to disambiguate which pronunciation is correct, which is a difficult problem.
    *   `because pronouncing it properly requires a deep voice`: **Incorrect.** This is a humorous distractor.
    *   `because they are infrequent words...`: **Incorrect.** "Bass" is a reasonably common word.
    *   `because double consonants like "ss" are harder...`: **Incorrect.** Double consonants are generally not a major problem for modern TTS systems.

---

### **Question 56**
**Question:** The study of patterns of stress and intonation (pitch) that affect the intended meaning of spoken language is referred to as:
**Correct Answer:** prosody

*   **Analysis:**
    *   **Correct:** This is the definition of **prosody**. It encompasses rhythm, stress, and intonation in speech and is crucial for conveying meaning beyond the words themselves (e.g., distinguishing a question from a statement, or indicating sarcasm).
    *   `anthropomorphism`: **Incorrect.** Attributing human qualities to non-humans.
    *   `semantics`: **Incorrect.** The study of meaning in language.
    *   `ontology`: **Incorrect.** A formal representation of knowledge.
    *   `dependency grammar`, `lexicon`, `morphology`, `syntax`: **Incorrect.** These are other branches of linguistics focusing on grammar, words, word-formation, and sentence structure, respectively.

---

### **Question 57**
**Question:** Begin-Inside-Outside tagging is often used for:
**Correct Answer:** annotating training sets for Named Entity Extraction tasks

*   **Analysis:**
    *   **Correct:** The **BIO (or IOB) tagging scheme** is the standard way to format training data for sequence labeling tasks that involve identifying "chunks" or "spans" of text. For NER, a sentence like "Jean-Luc Picard visited Paris" would be tagged:
        *   `Jean-Luc`: B-PER (Begin Person)
        *   `Picard`: I-PER (Inside Person)
        *   `visited`: O (Outside)
        *   `Paris`: B-LOC (Begin Location)
    *   This scheme allows the model to learn not just the entity type but also its boundaries.

---

### **Question 58**
**Question:** Imagine providing the following prompt to a large language model: `"I'm afraid for the calendar... => funny ... I asked my dog what's two minus two. He said nothing. =>"` What type of learning is the model doing in this case?
**Correct Answer:** few-shot learning

*   **Analysis:** The prompt provides several complete examples of a task (joke -> funny/not funny) before asking the model to complete a new instance.
    *   1st example: `calendar` joke -> `funny`
    *   2nd example: `alphabet` joke -> `funny`
    *   3rd example: `bowtie` joke -> `not funny`
    *   4th example: `factory` joke -> `not funny`
    *   5th example: `dryer` joke -> `funny`
    *   Since multiple (more than one) examples are provided in the prompt, this is **few-shot learning**.

---

### **Question 59**
**Question:** Do LLMs sometimes generate statements that are factually incorrect? If so, why? And if not, why not?
**Correct Answer:** Yes, because the LLM generates text one token at a time conditioned on previous tokens and there is no guarantee that the resulting text will not contain factual inaccuracies.

*   **Analysis:** This phenomenon is known as "hallucination."
    *   **Correct:** LLMs are probabilistic sequence models. They are trained to generate plausible-sounding text, not to state facts. Their architecture is optimized for predicting the next most likely token, not for verifying the truth of the entire sequence against a knowledge base. Factual correctness is an emergent property that is often, but not always, present.
    *   `Yes, because LLM stands for Lying Language Model`: **Incorrect.** Humorous distractor.
    *   `Yes, because the LLM has been trained to copy human behaviour...`: **Partially true, but not the core reason.** While training data contains lies, the fundamental reason for hallucination is architectural and objective-based, not just mimicry.
    *   `No, because they have been trained to never tell lies`: **Incorrect.** This is false.
    *   `Yes, because the LLM can often be mistaken in its belief...`: **Incorrect.** This anthropomorphizes the model. An LLM doesn't have "beliefs."

---

### **Question 60**
**Question:** Providing fast Nearest Neighbour search in a document embedding space is the task of what technology?
**Correct Answer:** Vector databases

*   **Analysis:**
    *   **Correct:** When documents are represented as high-dimensional embedding vectors, finding the "most similar" documents for a query vector becomes a **Nearest Neighbor Search** problem. **Vector databases** are specialized databases designed to store and index high-dimensional vectors to perform this search efficiently (using algorithms like HNSW, IVF, etc.).
    *   `BM25`: **Incorrect.** A lexical search algorithm, not for vector spaces.
    *   `LLMs`: **Incorrect.** LLMs *produce* embeddings, but they don't perform the fast search themselves.
    *   `Relational databases`, `NoSQL databases`: **Incorrect.** These are not optimized for high-dimensional vector search.

---

### **Question 61**
**Question:** In NLP, which of the following statements regarding a parse tree is NOT correct?
**Correct Answer:** a parse tree is used to convert audio signals into text in speech-to-text systems

*   **Analysis:**
    *   **Correct (as the false statement):** A parse tree represents the syntactic structure of a *text sentence*. Converting audio to text is the job of an Automatic Speech Recognition (ASR) system, which involves signal processing and acoustic modeling, not syntactic parsing of the final text.
    *   `a parse tree provides a tree structure over the words...`: **True.** This is its definition.
    *   `a parse tree provides a representation that tells us who did what...`: **True.** By identifying subject, verb, and object, it reveals the semantic roles.
    *   `a parse tree results from applying a formal grammar...`: **True.** Parsing is the process of applying grammar rules to a sentence to generate the tree.

---

### **Question 62**
**Question:** In a traditional (lexical/term-based) search engine, a posting list contains:
**Correct Answer:** mappings from term to documents

*   **Analysis:** The core data structure of a search engine is the **inverted index**.
    *   An inverted index is a mapping from **terms** (words) to their **posting lists**.
    *   A **posting list** for a given term is the list of all **documents** that contain that term.
    *   Therefore, the overall structure maps terms to documents.

---

### **Question 63**
**Question:** Which of the following statements about the Bag-of-Words (BOW) representation of a document is correct:
**Correct Answer:** A BOW vector is a summation one-hot-vectors over each word position in the document

*   **Analysis:**
    *   **Correct:** This is a good way to conceptualize the creation of a BoW vector. If you take the one-hot vector for each word in a document and sum them up, the resulting vector will have, at each position corresponding to a vocabulary word, the count of how many times that word appeared.
    *   `BOW vectors deal well with synonyms...`: **Incorrect.** This is a major weakness of BoW. "Car" and "automobile" are treated as completely different, unrelated features.
    *   `Storing each BOW vector requires space equal to the vocabulary size...`: **Incorrect.** BoW vectors are highly sparse. Efficient formats (like dictionaries or sparse matrices) are used that only store the non-zero entries, requiring much less space.
    *   `The number of dimensions of a BOW vector grows with the length of the document`: **Incorrect.** The number of dimensions is fixed and is equal to the size of the *vocabulary*, regardless of document length.
    *   `A BOW vector provides a dense representation...`: **Incorrect.** It is the canonical example of a *sparse* representation.

---

### **Question 64**
**Question:** In Information Theory, the logarithm of one on the probability of an event corresponds to:
**Correct Answer:** the amount of information learnt from the event

*   **Analysis:**
    *   **Correct:** This is the definition of **self-information** or **surprisal**. The formula is `I(x) = -log(P(x))`, which is equivalent to `log(1/P(x))`. An event with low probability (high surprise) yields a large amount of information when it occurs. An event with high probability (low surprise) yields little information.
    *   The other options are incorrect interpretations.

---

### **Question 65**
**Question:** Which of the following is a character encoding standard that can representing characters from many different languages?
**Correct Answer:** UTF-8

*   **Analysis:**
    *   **Correct:** **UTF-8** (Unicode Transformation Format) is the dominant character encoding for the web. It is a variable-width encoding that can represent every character in the Unicode standard, covering almost all of the world's writing systems.
    *   `Byte-pair encoding`: **Incorrect.** This is a tokenization algorithm, not a character encoding standard.
    *   `FastText`: **Incorrect.** An algorithm for learning word embeddings.
    *   `ASCII`: **Incorrect.** A 7-bit encoding that can only represent 128 characters, primarily for English. It cannot handle characters from other languages.

---

### **Question 66**
**Question:** In order to speed up model training, the Transformer model REMOVED what part of the sequence-to-sequence with attention model architecture?
**Correct Answer:** the recurrent links within the encoder and decoder

*   **Analysis:** The title of the original Transformer paper is "Attention Is All You Need."
    *   **Correct:** The key innovation of the Transformer was to rely *entirely* on attention mechanisms, completely removing the **recurrent (sequential) processing** of RNNs/LSTMs. This lack of recurrence allowed for massive parallelization of computations over the sequence tokens, dramatically speeding up training on modern hardware (GPUs/TPUs).
    *   The Transformer *kept* the encoder network, the decoder network, the attention mechanism connecting them, normalization layers, and feedforward networks. It just replaced the recurrent nature of the encoder/decoder with self-attention.

---

### **Question 67**
**Question:** Which of the following is NOT a property of Word2Vec word embeddings?
**Correct Answer:** the sum of the embeddings of the characters that are present in a word is equal to the embedding of the word itself

*   **Analysis:**
    *   **Correct (as the false statement):** This describes the mechanism of **FastText**, not Word2Vec. Word2Vec treats words as atomic units and learns a single vector for each. FastText represents a word as its vector plus the sum of the vectors of its character n-grams.
    *   `linear translations... can encode morphological transformations`: **True.** This is a known property (e.g., `vec(king) - vec(man) + vec(woman) ≈ vec(queen)`).
    *   `semantically related concepts tend to occur close...`: **True.** This is the primary goal and property.
    *   `semantic relationships (such as "capitol-city of") can be extracted...`: **True.** This is another example of vector analogies.
    *   `for words with multiple possible meanings... all... meanings are encoded in the word vector`: **True.** This is actually a *limitation* of standard Word2Vec. A single vector for a polysemous word like "bank" is an average of the vectors for all its contexts (river bank, financial bank), making it less precise.

---

### **Question 68**
**Question:** Should we prefer language models with higher perplexity or lower perplexity?
**Correct Answer:** lower perplexity

*   **Analysis:**
    *   **Correct:** **Perplexity** is a measure of how well a probability model predicts a sample. It is essentially the (exponentiated) average negative log-likelihood. A lower perplexity means the model is less "surprised" by the test data, meaning it assigns a higher probability to it. Therefore, **lower perplexity is better**.
    *   A perplexity of `k` can be loosely interpreted as the model being as confused as if it had to choose uniformly from `k` options at each step. We want this number to be as low as possible.

---

### **Question 69**
**Question:** If the output of a text classifier produces the following confusion matrix... What is the Precision of the classifier?
**Correct Answer:** 95%

*   **Analysis:**
    *   **Confusion Matrix:**
        ```
              Predicted
              +    -
        Actual +   95   25   (TP=95, FN=25)
               -    5   75   (FP=5,  TN=75)
        ```
    *   **Precision Formula:** Precision for the positive class is `TP / (TP + FP)`. This answers the question: "Of all the items we *predicted* as positive, how many were *actually* positive?"
    *   **Values:** True Positives (TP) = 95. False Positives (FP) = 5.
    *   **Calculation:** `Precision = 95 / (95 + 5) = 95 / 100 = 0.95`.
    *   This equals **95%**.

---

### **Question 70**
**Question:** Which of the following is NOT a common ranking function used in term-based information retrieval:
**Correct Answer:** NDCG

*   **Analysis:**
    *   **Correct:** **NDCG (Normalized Discounted Cumulative Gain)** is an **evaluation metric** used to measure the quality of a set of ranked results. It is not a *ranking function* used by the search engine itself to produce the ranking.
    *   `BM25`, `IDF cosine`, `TF-IDF cosine`: **Incorrect.** These are all common ranking functions (or components thereof) used in lexical search systems to score documents against a query.

---

### **Question 71**
**Question:** Which statement about a Long Short-Term Memory (LSTM) network is NOT true:
**Correct Answer:** due to its architecture it is not possible to stack LSTMs on top of each other

*   **Analysis:**
    *   **Correct (as the false statement):** It is very common to build **stacked LSTMs** (or deep LSTMs), where the output sequence of one LSTM layer is used as the input sequence for the next. This allows the network to learn representations at different levels of abstraction.
    *   `an LSTM is a type of recurrent neural network...`: **True.** It's a specific, advanced type of RNN designed to handle long-range dependencies.
    *   `an LSTM is a network with an ability to forget information`: **True.** The "forget gate" is a key component of the LSTM cell that controls what information from the previous state should be discarded.
    *   `an LSTM is a network that can learn what information to remember...`: **True.** The combination of the input, forget, and output gates allows the network to learn to control its own memory cell.

---

### **Question 72**
**Question:** The fact that the exclamation mark '!' can denote a factorial..., the question mark '?' can indicate a missing value..., and the period '.' can be a decimal point..., complicates which NLP task?
**Correct Answer:** Sentence segmentation

*   **Analysis:**
    *   **Correct:** **Sentence segmentation** (or sentence boundary detection) is the task of splitting a text into its constituent sentences. The most common heuristic is to split on punctuation like '.', '?', and '!'. However, these characters have other, non-sentence-ending meanings (e.g., `Mr. Smith`, `pi is approx. 3.14`, `What?!`). This ambiguity makes robust sentence segmentation a non-trivial task.
    *   The other tasks (POS tagging, NER, etc.) are also affected by punctuation, but the primary and most direct complication is for deciding where a sentence ends.

---

### **Question 73**
**Question:** Zero, one and few-shot learning involves
**Selected Answer (marked incorrect):** NONE of the other options is correct
**Likely Correct Answer:** using an existing model to perform a task without fine-tuning the model for the specific task

*   **Analysis:** This family of techniques refers to how a pre-trained model is prompted, specifically without updating its weights.
    *   `using an existing model to perform a task without fine-tuning...`: **Correct.** Zero, one, and few-shot learning are forms of **in-context learning**. The "learning" happens within the prompt's context window, not by changing the model's parameters via gradient descent (which is what "fine-tuning" means).
    *   `performing zero, one or few passes (epochs) over the training data`: **Incorrect.** This describes the number of epochs during *fine-tuning*, which is explicitly what is *not* happening.
    *   `fine-tuning on small batches of data...`: **Incorrect.** This again describes fine-tuning.
    *   `training a model to shoot at a particular target language...`: **Incorrect.** This is a garbled metaphor.

---

### **Question 74**
**Question:** Which statement about sequence-to-sequence models is true?
**Correct Answer:** they consist of two sub-models (usually recurrent neural networks): an encoder and a decoder

*   **Analysis:**
    *   **Correct:** This describes the fundamental architecture of a seq2seq model. An **encoder** reads the input sequence and compresses it into a context representation (a vector or a set of vectors). A **decoder** then uses this representation to generate the output sequence one token at a time. While Transformers are now common, the original and classic seq2seq models used RNNs.
    *   `not as powerful as sequence-labelling models`: **Incorrect.** They are designed for different, often more complex tasks (like translation) where input and output lengths can differ.
    *   `the input sequence and output sequence should be the same length`: **Incorrect.** A key advantage of seq2seq models is that they can handle inputs and outputs of different lengths.
    *   `corresponding words in the two languages should be in the same order`: **Incorrect.** They are specifically designed (especially with attention) to handle word reordering between languages.

---

### **Question 75**
**Question:** CLIP is a model primarily used for:
**Correct Answer:** aligning the embedding spaces of images and textual documents, so that they can be compared and retrieved

*   **Analysis:**
    *   **Correct:** **CLIP (Contrastive Language–Image Pre-training)** is a multi-modal model trained to learn a shared embedding space for images and text. It learns to map a given image and its correct textual description to nearby points in this space. This allows for powerful zero-shot image classification (by comparing an image to the embeddings of text labels like "a photo of a dog") and text-to-image retrieval.
    *   The other options describe various text-only NLP tasks.

---

### **Question 76**
**Question:** In order to improve the probability estimates for an n-gram language model we could:
**Correct Answer:** ALL of the other options are correct

*   **Analysis:** All the listed options are standard **smoothing** techniques used to handle the data sparsity problem in n-gram models (i.e., to avoid assigning zero probability to unseen n-grams).
    *   `smooth all probability estimates by adding a small constant...`: **Correct.** This is **additive smoothing** (or Laplace smoothing).
    *   `back-off the estimator to use lower-order n-grams...`: **Correct.** This is **back-off smoothing** (e.g., Katz back-off). If a high-order n-gram is unseen, it "backs off" to a lower-order model.
    *   `interpolate the higher-order probability estimates with lower-order estimates`: **Correct.** This is **interpolation**, where the final probability is a weighted average of the estimates from different n-gram orders.

---

### **Question 77**
**Question:** The main disadvantage of the k-Medoids algorithm with respect to the k-Means algorithm is:
**Correct Answer:** the need to compute a much larger number of distances per iteration of the algorithm

*   **Analysis:**
    *   **k-Means:** In each iteration, it computes the distance from every point to `k` centroids. Then it re-computes `k` new centroids (means), which is a fast operation.
    *   **k-Medoids:** In each iteration, after assigning points to the current medoids, it must find the new best medoid for each cluster. To do this, for each cluster, it must try *every single point in that cluster* as a potential new medoid and calculate the total distance from all other points in the cluster to it. This is computationally much more expensive than just calculating a mean.
    *   **Correct:** This computational complexity (`O(k(n-k)^2)` in a naive implementation) is the primary drawback of k-Medoids compared to k-Means.

---

### **Question 78**
**Question:** Which of the following is NOT an evaluation measure commonly used to evaluate a search system
**Correct Answer:** Silhouette Index (SI)

*   **Analysis:**
    *   **Correct:** The **Silhouette Index** is an evaluation metric for **clustering** algorithms. It measures how well-separated and compact the clusters are. It has no application in evaluating a ranked search result list.
    *   `Mean Average Precision (MAP)`, `Precision at rank k (P@k)`, `Normalised Discounted Cumulative Gain (NDCG)`: **Incorrect.** These are all cornerstone metrics for evaluating information retrieval and search engine performance.

---

### **Question 79**
**Question:** [CLS] and [SEP] tokens are often used with which type of model and for doing what?
**Correct Answer:** with BERT models for pairwise classification tasks

*   **Analysis:**
    *   **Correct:** **BERT** uses special tokens to structure its input.
        *   `[CLS]`: This token is prepended to every input sequence. The final hidden state corresponding to this token is used as the aggregate sequence representation for classification tasks.
        *   `[SEP]`: This token is used to separate sentences. For a pairwise task like Question Answering or Natural Language Inference, the input is formatted as `[CLS] sentence_A [SEP] sentence_B [SEP]`.
    *   This structure is unique to BERT-like models and is essential for their classification capabilities.

---

### **Question 80**
**Question:** Assume that you have learnt Word2Vec embeddings of size 512 over a vocabulary of four hundred thousand tokens... how much memory (in GB) would you need to store all of the vectors if... double precision (64 bit) floating point numbers are used?
**Correct Answer:** 1.6 GB

*   **Analysis:**
    1.  **Vocabulary Size:** 400,000
    2.  **Embedding Dimension (size):** 512
    3.  **Precision:** double precision = 64 bits.
    4.  **Bits to Bytes:** 64 bits = 8 bytes.
    5.  **Memory per Vector:** `512 dimensions * 8 bytes/dimension = 4096` bytes.
    6.  **Total Memory in Bytes:** `400,000 vectors * 4096 bytes/vector = 1,638,400,000` bytes.
    7.  **Bytes to Gigabytes (GB):** 1 GB ≈ 10^9 bytes.
    8.  **Calculation:** `1,638,400,000 bytes ≈ 1.6384 GB`.
    9.  The closest answer is **1.6 GB**.

---

### **Question 81**
**Question:** Which of the following statements about the uses of word embeddings is generally true?
**Correct Answer:** ALL of the other statements are generally true

*   **Analysis:** Word embeddings have had a transformative impact on NLP.
    *   `pre-trained word embeddings are available... and can be downloaded...`: **True.** This is a major reason for their popularity. Models like GloVe and FastText provide readily available pre-trained vectors.
    *   `word embeddings are useful for dealing with problems of synonymy...`: **True.** This is a key advantage over BoW. Synonyms (e.g., "car", "auto") will have similar vectors, allowing a model to generalize between them.
    *   `word embeddings have been used to mine scientific literature...`: **True.** A famous example is using Word2Vec on materials science papers to predict new thermoelectric materials.
    *   `word embeddings tend to improve the performance on any NLP task`: **True.** Using pre-trained embeddings as the input layer for a neural network almost always provides a significant performance boost over random initialization or one-hot vectors.

---

### **Question 82**
**Question:** The image shown demonstrates that:
**Correct Answer:** Translation model can take into account noun gender when translating articles (like "the", "a", etc.).

*   **Analysis:** Without seeing the image, we must infer its content from the correct answer. A common example image for this concept shows an English sentence like "The European Economic Area" being translated into French.
    *   In French, "Area" (`Zone`) is a feminine noun.
    *   The translation would correctly be "**La** Zone économique européenne".
    *   The model correctly chose the feminine article "La" instead of the masculine "Le", demonstrating that the attention mechanism allowed it to use information about the noun "Zone" when choosing the article, even though they are separated in the sentence. This highlights the model's ability to handle grammatical agreement like noun gender.

---

### **Question 83**
**Question:** Consider a bigram language model... Using top-k sampling with k set to 2, what would the chance of seeing the output "a b c" be?
**Correct Answer:** 0

*   **Analysis:** We calculate the probability of the sequence step-by-step using top-k=2 sampling.
    *   **P(a):** The initial probabilities are P(a)=1/2, P(b)=1/4, P(c)=1/8, P(d)=1/8. The top 2 are 'a' and 'b'. 'a' is a possible first word.
    *   **P(b|a):** Given 'a', the probabilities are P(b|a)=1/2, P(c|a)=1/2. The top 2 are 'b' and 'c'. 'b' is a possible second word.
    *   **P(c|b):** Given 'b', the probabilities are P(b|b)=1/4 (0.25), P(d|b)=1/4 (0.25), P(a|b)=1/8 (0.125), P(c|b)=1/8 (0.125).
        *   The two most probable next words (the top-k=2 set) are 'b' and 'd'.
        *   The word 'c' is **not** in the top 2 candidates.
    *   **Conclusion:** Since the probability of generating 'c' after 'b' is 0 under this sampling scheme, the probability of the entire sequence "a b c" is `P(a) * P(b|a) * 0 = 0`.

---

### **Question 84**
**Question:** Which of the following techniques is often used for learning sequence-to-sequence models in NLP?
**Correct Answer:** Recurrent neural networks

*   **Analysis:**
    *   **Correct:** As described in Q74, the classic and foundational architecture for seq2seq models is to use one **Recurrent Neural Network (RNN)** (often an LSTM or GRU) as the encoder and another as the decoder. While Transformers are now more common, RNNs are still fundamentally associated with seq2seq learning.
    *   The other options are all distinct ML/NLP techniques not directly used for building the core of a seq2seq model.

---

### **Question 85**
**Question:** Which of the following is a common text pre-processing step in an NLP application?
**Correct Answer:** ALL of the other options are common preprocessing activities

*   **Analysis:**
    *   `Tokenization`: The first step in almost any NLP pipeline, splitting text into words or sub-words.
    *   `Removing HTML markup`: Essential when processing text from the web.
    *   `Lemmatization or stemming`: Common normalization steps to reduce vocabulary size.
    *   `Stopword removal`: A very common step to remove non-informative words.
    *   `Case-folding`: Converting all text to a single case (usually lowercase).
    *   All of these are standard, common preprocessing techniques.

---

### **Question 86 (Open-ended System Design)**
**Question:** Your company wants to build a voice interactive system to control a washing machine... What tools and data would you need to use to build it? How would you go about developing the system?
**Score:** 4/5 Punti (implying a good but incomplete answer from the user)

*   **A full-credit (5/5) answer would likely include:**
    1.  **System Architecture Breakdown:** Clearly identifying all necessary NLP/ML components.
        *   **Input:** Automatic Speech Recognition (ASR) to convert voice to text.
        *   **Understanding:** Natural Language Understanding (NLU) to find the user's intent (e.g., `set_wash_cycle`) and entities/slots (e.g., `fabric: cotton`, `temperature: 60`, `delay: 2_hours`).
        *   **Context/Logic:** A Dialog State Tracker (DST) to remember the conversation's context (e.g., the user has specified the fabric but not the temperature) and a Dialog Policy Manager to decide the next action (e.g., `ask_for_temperature`).
        *   **Advisory Component:**
            *   A **Computer Vision (CV)** model to analyze clothes (fabric type, color mix) from the camera.
            *   A backend logic/rules engine to provide advice based on CV output, weight sensor data, and user input (e.g., "Warning: mixing darks and lights is not recommended.").
        *   **Output:** Natural Language Generation (NLG) to create a text response, and Text-to-Speech (TTS) to speak it.
    2.  **Data & Tools for Each Component:**
        *   **ASR:** Tool (e.g., fine-tuning Whisper, using Google Speech-to-Text API). **Data (Crucial):** Need to collect and transcribe thousands of audio samples of users giving washing machine commands in various accents and phrasings ("wash my woolies," "put on a 60-degree wash," etc.) to fine-tune the ASR model for better domain-specific accuracy.
        *   **NLU:** Tool (e.g., Rasa, Dialogflow, or a custom BERT-based classifier). **Data:** A labeled dataset of text commands mapped to intents and entities. This would need to be created by hand.
        *   **CV:** Tool (e.g., PyTorch/TensorFlow with a pre-trained ResNet/MobileNet). **Data:** A large, custom-collected, and labeled image dataset of various clothes, fabrics, and common laundry loads.
        *   **TTS:** Tool (e.g., Amazon Polly, Google TTS API). Usually requires no special data, just selecting a suitable voice.
    3.  **Development & Deployment Strategy:**
        *   **Prototyping (Wizard of Oz):** Start with a human pretending to be the system to quickly gather realistic user interaction data.
        *   **Data Collection:** The biggest task. Run user studies to collect the necessary audio, text, and image data.
        *   **Iterative Development:** Build the system in phases.
            *   Phase 1: Basic command-and-control (ASR -> NLU -> Action).
            *   Phase 2: Add multi-turn dialog context (DST & Policy).
            *   Phase 3: Integrate the advisory system (CV model & rules engine).
        *   **Testing:** Rigorous end-to-end testing with real users to find failure points (e.g., ASR errors, NLU confusion) and use that data to improve the models.
